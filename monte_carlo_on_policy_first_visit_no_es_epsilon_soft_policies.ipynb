{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "monte-carlo_on-policy_first-visit_no-es_epsilon-soft-policies.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcoPlWetwKi7",
        "colab_type": "text"
      },
      "source": [
        "# Monte Carlo\n",
        "## On-policy first-visit MC control (for epsilon-soft policies)\n",
        "\n",
        "- Rodolfo Vasconcelos\n",
        "\n",
        "SCS 3547 – Intelligent Agents & Reinforcement Learning\n",
        "\n",
        "UNIVERSITY OF TORONTO - SCHOOL OF CONTINUING STUDIES\n",
        "\n",
        "Instructor: Larry Simon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lJKF1jhy8dB",
        "colab_type": "text"
      },
      "source": [
        "## About the Project\n",
        "Implementation of the algorithm given on Chapter 5.4, page 101 of Sutton & Barton's book \"Reinforcement Learning: An Intruduction\", which is the On-policy first-visit Mont Carlo control (for epsilon-soft policies).\n",
        "\n",
        "![On-policy first-visit MC control](https://raw.githubusercontent.com/ravasconcelos/monte_carlo/master/images/onpolicy_firstvisit_MC_esoft.png)\n",
        "\n",
        "This algorithm to find an approximation of the optimal policy for the gridworld on page 76 and 77 of the book above.\n",
        "\n",
        "![Optimal Policy](https://raw.githubusercontent.com/ravasconcelos/monte_carlo/master/images/optimal_policy.png)\n",
        "\n",
        "This notebook prints as output a table of the estimated q function Q(s,a) for the optimal policy and the optimal policy itself.\n",
        "\n",
        "**Note**: this code was based on the Lazy Programmer's code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC2KqAJA0FMp",
        "colab_type": "text"
      },
      "source": [
        "# Algorithm\n",
        "This Monte Carlo algorithm produces n episodes starting from random points of the grid, and let the agent move to the four directions according to the epsilon-soft policy until a termination state is achieved. \n",
        "\n",
        "For each episode we save the 4 values: (1) the initial state, (2) the action taken, (3) the reward received and (4) the final state. \n",
        "\n",
        "In the end, a episode is just an array containing x arrays of these values, x being the number of steps the agent had to take until reaching a terminal state.\n",
        "\n",
        "From these episodes, we iterate from the end of the “experience” array, and compute G as the previous state value in the same experience (weighed by gamma, the discount factor) plus the received reward in that state. \n",
        "\n",
        "For each pair St,At that is not in the experience array:\n",
        "* append G in an array of Returns(St,At)\n",
        "* compute the Q(St,At) as the average of the Returns(St,At)\n",
        "* get the action for the max Q(St,At)\n",
        "* update the pi(a|St)\n",
        "\n",
        "pi(a|St) contais the probalities an action can be chosen for a given state.Changing the epsilon we can change the probability the agent will be able to explore other actions insteady of choosing the best action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNcWjzAV1pFg",
        "colab_type": "text"
      },
      "source": [
        "## Constants\n",
        "A good approximation of the best policy can be found with:\n",
        "* Number of episodes: 5000\n",
        "* Gamma=0.6\n",
        "* Epsilon=0.3\n",
        "\n",
        "Or:\n",
        "* Number of episodes: 10000\n",
        "* Gamma=0.9\n",
        "* Epsilon=0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAlHdXAnCklG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPISODES = 5000 # number of episodes\n",
        "GAMMA = 0.6\n",
        "EPS = 0.4\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGWGgI8l0Jam",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ltt74CEB2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NipSE8kEEQx2",
        "colab_type": "text"
      },
      "source": [
        "## Print functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvm3ut1HEWX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_Q(Q, grid):\n",
        "    print('----------------------------------------------')\n",
        "    print('| State  | U      | D      | L      | R      |')\n",
        "    print('----------------------------------------------')\n",
        "    all_states = sorted(grid.all_states())\n",
        "    for state in all_states:\n",
        "        print(\"| {} |\".format(state), end=\"\")\n",
        "        if state != (0,0) and state != (3,3):\n",
        "            for action, value in Q[state].items():\n",
        "                print(' {:>6.2f} |'.format(value), end=\"\")\n",
        "        else:\n",
        "            print('   0.00 |   0.00 |   0.00 |   0.00 |',end=\"\")\n",
        "        print('')\n",
        "    print('----------------------------------------------')\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZfCIaCEdx2",
        "colab_type": "text"
      },
      "source": [
        "## Grid World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_kLB0HAFMog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Grid: # Environment\n",
        "  def __init__(self, rows, cols, start):\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
        "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self, s):\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    return (self.i, self.j)\n",
        "\n",
        "  def is_terminal(self, s):\n",
        "    return s not in self.actions\n",
        "\n",
        "  \n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    #print('before i={} j={} action={}'.format(self.i,self.j,action))\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    # return a reward (if any)\n",
        "    #return self.rewards.get((self.i, self.j), 0)\n",
        "    reward = self.rewards.get((self.i, self.j), 0)\n",
        "    #print('after i={} j={} r={}'.format(self.i,self.j,reward))\n",
        "    return reward\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    # these are the opposite of what U/D/L/R should normally do\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    # raise an exception if we arrive somewhere we shouldn't be\n",
        "    # should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "\n",
        "  def game_over(self):\n",
        "    # returns true if game is over, else false\n",
        "    # true if we are in a state where no actions are possible\n",
        "    return (self.i, self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    # possibly buggy but simple way to get all states\n",
        "    # either a position that has possible next actions\n",
        "    # or a position that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
        "\n",
        "\n",
        "def standard_grid():\n",
        "  # define a grid that describes the reward for arriving at each state\n",
        "  # and possible actions at each state\n",
        "  # the grid looks like this\n",
        "  # S means start position\n",
        "  # E means the end states\n",
        "  #\n",
        "  # E  .  .  .\n",
        "  # .  .  . .\n",
        "  # S  .  .  .\n",
        "  # .  .  .  E\n",
        "  g = Grid(4, 4, (2, 0))\n",
        "  rewards = {(0, 0): 0, (3, 3): 0}\n",
        "  actions = {\n",
        "    #(0, 0): (), End-State\n",
        "    (0, 1): ('D', 'R', 'L'),\n",
        "    (0, 2): ('D', 'R', 'L'),\n",
        "    (0, 3): ('D', 'L'),\n",
        "    (1, 0): ('D', 'R', 'U'),\n",
        "    (1, 1): ('D', 'R', 'L', 'U'),\n",
        "    (1, 2): ('D', 'R', 'L', 'U'),\n",
        "    (1, 3): ('D', 'U', 'L'),\n",
        "    (2, 0): ('D', 'U', 'R'),\n",
        "    (2, 1): ('D', 'R', 'L', 'U'),\n",
        "    (2, 2): ('D', 'R', 'L', 'U'),\n",
        "    (2, 3): ('D', 'U', 'L'),\n",
        "    (3, 0): ('U', 'R', ),\n",
        "    (3, 1): ('U', 'R', 'L'),\n",
        "    (3, 2): ('U', 'R', 'L'),\n",
        "    #(3, 3): (), End-State\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g\n",
        "\n",
        "\n",
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (0, 3): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 1): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (1, 3): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "    (3, 0): step_cost,\n",
        "    (3, 1): step_cost,\n",
        "    (3, 2): step_cost,\n",
        "  })\n",
        "  return g"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ2VSwUFdzn",
        "colab_type": "text"
      },
      "source": [
        "## pi functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG56WfXqFmeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "\n",
        "def policy_using_pi(St, pi):\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi[(St,a)] for a in ALL_POSSIBLE_ACTIONS])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKL7eAdHFop3",
        "colab_type": "text"
      },
      "source": [
        "## Episode functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_8Aud69FrQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episode(grid, policy, pi):\n",
        "  # returns a list of states and corresponding returns\n",
        "  # in this version we will NOT use \"exploring starts\" method\n",
        "  # instead we will explore using an epsilon-soft policy\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  a = policy_using_pi(s,pi)\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    if grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = policy_using_pi(s,pi)\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioTLpq8AF_tt",
        "colab_type": "text"
      },
      "source": [
        "## Run all episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my5DDS3Cv1o-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cfb9bcd-4ab6-4804-bd28-55d338fbc3a7"
      },
      "source": [
        "# use the standard grid again (0 for every step) so that we can compare\n",
        "# to iterative policy evaluation\n",
        "# grid = standard_grid()\n",
        "# try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "# in order to minimize number of steps\n",
        "grid = negative_grid(step_cost=-1)\n",
        "\n",
        "# print rewards\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)\n",
        "\n",
        "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))  # probability of action (def random)\n",
        "\n",
        "# state -> action\n",
        "# initialize a random policy\n",
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "# initialize Q(s,a) and returns\n",
        "Q = {}\n",
        "returns = {} # dictionary of state -> list of returns we've received\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = -10\n",
        "        returns[(s,a)] = []\n",
        "else:\n",
        "    # terminal state or state we can't otherwise get to\n",
        "    pass\n",
        "\n",
        "#print(\"initial Q:\")\n",
        "#print_Q(Q,grid)\n",
        "\n",
        "# repeat until convergence\n",
        "deltas = []\n",
        "for t in range(EPISODES):\n",
        "    if t % 1000 == 0:\n",
        "        print(t)\n",
        "        print(\"Q:\")\n",
        "        print_Q(Q,grid)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_episode(grid, policy, pi)\n",
        "\n",
        "    # calculate Q(s,a)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "        # check if we have already seen s\n",
        "        # called \"first-visit\" MC policy evaluation\n",
        "        sa = (s, a)\n",
        "        if sa not in seen_state_action_pairs:\n",
        "            old_q = Q[s][a]\n",
        "            returns[sa].append(G)\n",
        "            Q[s][a] = np.mean(returns[sa])\n",
        "            biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "            seen_state_action_pairs.add(sa)\n",
        "            A_star, _ = max_dict(Q[s])\n",
        "            for a_index in ALL_POSSIBLE_ACTIONS:\n",
        "                if a_index == A_star:   pi[(s,a_index)] = 1 - EPS + EPS/len(ALL_POSSIBLE_ACTIONS)\n",
        "                else:                   pi[(s,a_index)] = EPS/len(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
        "    for s in policy.keys():\n",
        "        a, _ = max_dict(Q[s])\n",
        "        policy[s] = a\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00|-1.00|-1.00|-1.00|\n",
            "---------------------------\n",
            "-1.00|-1.00|-1.00|-1.00|\n",
            "---------------------------\n",
            "-1.00|-1.00|-1.00|-1.00|\n",
            "---------------------------\n",
            "-1.00|-1.00|-1.00| 0.00|\n",
            "0\n",
            "Q:\n",
            "----------------------------------------------\n",
            "| State  | U      | D      | L      | R      |\n",
            "----------------------------------------------\n",
            "| (0, 0) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "| (0, 1) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (0, 2) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (0, 3) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (1, 0) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (1, 1) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (1, 2) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (1, 3) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (2, 0) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (2, 1) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (2, 2) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (2, 3) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (3, 0) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (3, 1) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (3, 2) | -10.00 | -10.00 | -10.00 | -10.00 |\n",
            "| (3, 3) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "----------------------------------------------\n",
            "1000\n",
            "Q:\n",
            "----------------------------------------------\n",
            "| State  | U      | D      | L      | R      |\n",
            "----------------------------------------------\n",
            "| (0, 0) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "| (0, 1) |  -1.12 |  -1.86 |   0.00 |  -2.25 |\n",
            "| (0, 2) | -10.00 |  -2.08 | -10.00 |  -2.31 |\n",
            "| (0, 3) | -10.00 |  -2.18 | -10.00 | -10.00 |\n",
            "| (1, 0) |   0.00 |  -1.89 |  -1.32 |  -1.93 |\n",
            "| (1, 1) |  -1.35 |  -2.10 |  -1.24 |  -2.18 |\n",
            "| (1, 2) |  -2.38 |  -1.90 |  -2.38 |  -2.13 |\n",
            "| (1, 3) |  -2.31 |  -1.48 |  -2.05 | -10.00 |\n",
            "| (2, 0) |  -1.30 |  -2.22 |  -1.91 |  -2.12 |\n",
            "| (2, 1) |  -1.77 |  -2.14 |  -2.04 |  -1.87 |\n",
            "| (2, 2) |  -2.15 |  -1.25 |  -2.15 |  -1.51 |\n",
            "| (2, 3) |  -2.38 |   0.00 |  -1.70 | -10.00 |\n",
            "| (3, 0) |  -1.95 |  -2.19 |  -2.20 |  -2.09 |\n",
            "| (3, 1) |  -2.20 |  -1.96 |  -2.25 |  -1.33 |\n",
            "| (3, 2) |  -1.92 |  -1.26 |  -2.02 |   0.00 |\n",
            "| (3, 3) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "----------------------------------------------\n",
            "2000\n",
            "Q:\n",
            "----------------------------------------------\n",
            "| State  | U      | D      | L      | R      |\n",
            "----------------------------------------------\n",
            "| (0, 0) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "| (0, 1) |  -1.44 |  -1.83 |   0.00 |  -2.08 |\n",
            "| (0, 2) |  -1.60 |  -2.14 |  -1.22 |  -2.22 |\n",
            "| (0, 3) | -10.00 |  -2.11 | -10.00 |  -2.43 |\n",
            "| (1, 0) |   0.00 |  -1.91 |  -1.33 |  -1.92 |\n",
            "| (1, 1) |  -1.35 |  -2.18 |  -1.29 |  -2.14 |\n",
            "| (1, 2) |  -2.19 |  -1.88 |  -1.86 |  -2.13 |\n",
            "| (1, 3) |  -2.38 |  -1.65 |  -2.05 | -10.00 |\n",
            "| (2, 0) |  -1.31 |  -2.21 |  -1.92 |  -2.12 |\n",
            "| (2, 1) |  -1.84 |  -2.08 |  -1.97 |  -1.87 |\n",
            "| (2, 2) |  -2.15 |  -1.25 |  -2.17 |  -1.61 |\n",
            "| (2, 3) |  -2.15 |   0.00 |  -1.82 |  -1.00 |\n",
            "| (3, 0) |  -1.96 |  -2.23 |  -2.19 |  -1.99 |\n",
            "| (3, 1) |  -2.21 |  -1.93 |  -2.23 |  -1.31 |\n",
            "| (3, 2) |  -1.93 |  -1.22 |  -1.94 |   0.00 |\n",
            "| (3, 3) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "----------------------------------------------\n",
            "3000\n",
            "Q:\n",
            "----------------------------------------------\n",
            "| State  | U      | D      | L      | R      |\n",
            "----------------------------------------------\n",
            "| (0, 0) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "| (0, 1) |  -1.37 |  -1.83 |   0.00 |  -2.12 |\n",
            "| (0, 2) |  -1.74 |  -2.11 |  -1.31 |  -2.22 |\n",
            "| (0, 3) | -10.00 |  -2.04 | -10.00 |  -2.43 |\n",
            "| (1, 0) |   0.00 |  -1.90 |  -1.30 |  -1.91 |\n",
            "| (1, 1) |  -1.32 |  -2.14 |  -1.30 |  -2.13 |\n",
            "| (1, 2) |  -2.09 |  -1.88 |  -1.85 |  -1.97 |\n",
            "| (1, 3) |  -2.24 |  -1.43 |  -2.04 |  -2.18 |\n",
            "| (2, 0) |  -1.30 |  -2.21 |  -1.92 |  -2.13 |\n",
            "| (2, 1) |  -1.88 |  -2.05 |  -1.96 |  -1.88 |\n",
            "| (2, 2) |  -2.16 |  -1.26 |  -2.17 |  -1.53 |\n",
            "| (2, 3) |  -2.07 |   0.00 |  -1.82 |  -1.00 |\n",
            "| (3, 0) |  -1.96 |  -2.20 |  -2.17 |  -1.97 |\n",
            "| (3, 1) |  -2.21 |  -1.97 |  -2.25 |  -1.34 |\n",
            "| (3, 2) |  -1.91 |  -1.26 |  -2.00 |   0.00 |\n",
            "| (3, 3) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "----------------------------------------------\n",
            "4000\n",
            "Q:\n",
            "----------------------------------------------\n",
            "| State  | U      | D      | L      | R      |\n",
            "----------------------------------------------\n",
            "| (0, 0) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "| (0, 1) |  -1.26 |  -1.86 |   0.00 |  -2.13 |\n",
            "| (0, 2) |  -1.80 |  -2.11 |  -1.31 |  -2.21 |\n",
            "| (0, 3) |  -1.96 |  -1.99 | -10.00 |  -2.43 |\n",
            "| (1, 0) |   0.00 |  -1.92 |  -1.32 |  -1.91 |\n",
            "| (1, 1) |  -1.33 |  -2.13 |  -1.31 |  -2.14 |\n",
            "| (1, 2) |  -2.01 |  -1.89 |  -1.89 |  -1.98 |\n",
            "| (1, 3) |  -2.24 |  -1.41 |  -2.04 |  -2.18 |\n",
            "| (2, 0) |  -1.31 |  -2.20 |  -1.92 |  -2.12 |\n",
            "| (2, 1) |  -1.86 |  -2.02 |  -1.94 |  -1.88 |\n",
            "| (2, 2) |  -2.16 |  -1.28 |  -2.18 |  -1.55 |\n",
            "| (2, 3) |  -1.99 |   0.00 |  -1.83 |  -1.00 |\n",
            "| (3, 0) |  -1.94 |  -2.17 |  -2.16 |  -1.97 |\n",
            "| (3, 1) |  -2.21 |  -2.02 |  -2.21 |  -1.32 |\n",
            "| (3, 2) |  -1.91 |  -1.21 |  -1.95 |   0.00 |\n",
            "| (3, 3) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXi3LskXGaCN",
        "colab_type": "text"
      },
      "source": [
        "## Print results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxLw1RDPGni9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "6f0217ed-2193-4b2a-ea93-6ddf80a94d29"
      },
      "source": [
        "plt.plot(deltas)\n",
        "plt.show()\n",
        "\n",
        "# find the optimal state-value function\n",
        "# V(s) = max[a]{ Q(s,a) }\n",
        "V = {}\n",
        "for s in policy.keys():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "print(\"final values:\")\n",
        "print_values(V, grid)\n",
        "print(\"final policy:\")\n",
        "print_policy(policy, grid)\n",
        "print(\"final Q:\")\n",
        "print_Q(Q,grid)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final values:\n",
            "---------------------------\n",
            " 0.00| 0.00|-1.18|-1.91|\n",
            "---------------------------\n",
            " 0.00|-1.30|-1.87|-1.36|\n",
            "---------------------------\n",
            "-1.31|-1.87|-1.29| 0.00|\n",
            "---------------------------\n",
            "-1.95|-1.33| 0.00| 0.00|\n",
            "final policy:\n",
            "---------------------------\n",
            "     |  L  |  L  |  D  |\n",
            "---------------------------\n",
            "  U  |  L  |  L  |  D  |\n",
            "---------------------------\n",
            "  U  |  U  |  D  |  D  |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |     |\n",
            "final Q:\n",
            "----------------------------------------------\n",
            "| State  | U      | D      | L      | R      |\n",
            "----------------------------------------------\n",
            "| (0, 0) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "| (0, 1) |  -1.22 |  -1.87 |   0.00 |  -2.01 |\n",
            "| (0, 2) |  -1.74 |  -2.11 |  -1.18 |  -2.21 |\n",
            "| (0, 3) |  -2.22 |  -1.91 | -10.00 |  -2.43 |\n",
            "| (1, 0) |   0.00 |  -1.93 |  -1.30 |  -1.89 |\n",
            "| (1, 1) |  -1.31 |  -2.13 |  -1.30 |  -2.14 |\n",
            "| (1, 2) |  -1.93 |  -1.88 |  -1.87 |  -1.98 |\n",
            "| (1, 3) |  -2.23 |  -1.36 |  -2.04 |  -2.18 |\n",
            "| (2, 0) |  -1.31 |  -2.20 |  -1.92 |  -2.13 |\n",
            "| (2, 1) |  -1.87 |  -2.00 |  -1.93 |  -1.89 |\n",
            "| (2, 2) |  -2.15 |  -1.29 |  -2.20 |  -1.58 |\n",
            "| (2, 3) |  -2.00 |   0.00 |  -1.89 |  -1.49 |\n",
            "| (3, 0) |  -1.95 |  -2.18 |  -2.17 |  -1.97 |\n",
            "| (3, 1) |  -2.20 |  -1.94 |  -2.20 |  -1.33 |\n",
            "| (3, 2) |  -1.94 |  -1.28 |  -2.01 |   0.00 |\n",
            "| (3, 3) |   0.00 |   0.00 |   0.00 |   0.00 |\n",
            "----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC7UOIwQusZJ",
        "colab_type": "text"
      },
      "source": [
        "# References:\n",
        "\n",
        "* Gerard Martínez: [Reinforcement learning (RL) 101 with Python](https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b)\n",
        "*   Henry AI Labs: [Monte Carlo Methods - Reinforcement Learning Chapter 5](https://www.youtube.com/watch?v=uiPhlFrwcw8&t=68s)\n",
        "*   Lazy Programmer:[monte_carlo_no_es](https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/monte_carlo_no_es.py)\n",
        "*   Marcin Bogdanski: [On-Policy First-Visit MC Control](https://marcinbogdanski.github.io/rl-sketchpad/RL_An_Introduction_2018/0504_On_Policy_First_Visit_MC_Control.html)\n",
        "*   Sutton & Barton, 2018: [Reinforcement Learning: An Introduction. Second edition](https://www.amazon.ca/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249/ref=sr_1_1?dchild=1&qid=1595207619&refinements=p_27%3ARichard+S.+Sutton&s=books&sr=1-1&text=Richard+S.+Sutton)\n"
      ]
    }
  ]
}